Name: Spark-SQL
Config:
  spark.master: yarn
  spark.submit.deployMode: client
  spark.default.parallelism: 100
  spark.driver.cores: 4
  spark.driver.memory: 8192M
  spark.executor.id: driver
  spark.executor.cores: 4
  spark.executor.memory: 10240M
  spark.executor.defaultJavaOptions: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'
  spark.executor.extraLibraryPath: /usr/lib/hadoop-2.10.1/lib/native:/usr/lib/hadoop-2.10.1-lzo/lib/native
  spark.executor.extraClassPath: /usr/lib/hadoop-2.10.1-lzo/lib/*:/usr/lib/hadoop-2.10.1/hadoop-2.10.1-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-3.1.1-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-3.1.1-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-3.1.1-sdk/lib/sagemaker-spark-3.1.1-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-3.1.1-connector.jar
  spark.executorEnv.PYTHONPATH: /usr/lib/spark-3.1.1/python/lib/pyspark.zip:/usr/lib/spark-3.1.1/python/lib//py4j-0.10.7-src.zip
  spark.serializer.objectStreamReset: 100
  spark.eventLog.enabled: true
  spark.shuffle.service.enabled: true
  spark.rdd.compress: True
  spark.stage.attempt.ignoreOnDecommissionFetchFailure: true
  spark.scheduler.mode: FIFO
  spark.resourceManager.cleanupExpiredHost: true
  spark.eventLog.dir: hdfs:///var/log/spark-3.1.1/apps
  spark.dynamicAllocation.enabled: true
  spark.history.ui.port: 18080
  spark.history.fs.logDirectory: hdfs:///var/log/spark-3.1.1/apps
  spark.blacklist.decommissioning.enabled: true
  spark.decommissioning.timeout.threshold: 20
  spark.sql.warehouse.dir: hdfs://localhost:8020/user/spark-3.1.1/warehouse
  spark.sql.catalogImplementation: hive
