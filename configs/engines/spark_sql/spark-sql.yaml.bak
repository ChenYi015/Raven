Name: Spark-SQL
Config:
  spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem: 2
  spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds: 2000
  spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem: true
  spark.hadoop.yarn.timeline-service.enabled: false
  spark.yarn.dist.filesL: file:/etc/spark/conf/hive-site.xml
  spark.yarn.historyServer.address: localhost:18080
  spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS: $(hostname -f)
  spark.yarn.isPython: true
  spark.yarn.executor.memoryOverheadFactor: 0.1875
  spark.master: yarn
  spark.submit.deployMode: client
  spark.driver.memory: 8192M
  spark.driver.extraLibraryPath: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
  spark.driver.extraClassPath: /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
  spark.driver.defaultJavaOptions: -XX:OnOutOfMemoryError='kill -9 %p'
  spark.sql.warehouse.dir: hdfs://localhost:8020/user/spark/warehouse
  spark.sql.parquet.fs.optimized.committer.optimization-enabled: true
  spark.sql.parquet.output.committer.class: com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter
  spark.sql.emr.internal.extensions: com.amazonaws.emr.spark.EmrSparkSessionExtensions
  spark.sql.hive.metastore.sharedPrefixes: com.amazonaws.services.dynamodbv2
  spark.sql.catalogImplementation: hive
  spark.serializer.objectStreamReset: 100
  spark.eventLog.enabled: true
  spark.shuffle.service.enabled: true
  spark.rdd.compress: True
  spark.executor.id: driver
  spark.executor.cores: 4
  spark.executor.memory: 9486M
  spark.executor.defaultJavaOptions: -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'
  spark.executor.extraLibraryPath: /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native
  spark.executor.extraClassPath: /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar
  spark.executorEnv.PYTHONPATH: /usr/lib/spark/python/lib/pyspark.zip<CPS>/usr/lib/spark/python/lib//py4j-0.10.7-src.zip
  spark.stage.attempt.ignoreOnDecommissionFetchFailure: true
  spark.scheduler.mode: FIFO
  spark.files.fetchFailure.unRegisterOutputOnHost: true
  spark.ui.showConsoleProgress: true
  spark.resourceManager.cleanupExpiredHost: true
  spark.ui.filters: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter
  spark.blacklist.decommissioning.timeout: 1h
  spark.eventLog.dir: hdfs:///var/log/spark/apps
  spark.dynamicAllocation.enabled: true
  spark.history.ui.port: 18080
  spark.history.fs.logDirectory: hdfs:///var/log/spark/apps
  spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS: localhost
  spark.blacklist.decommissioning.enabled: true
  spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES: http://localhost:20888/proxy/application_1637588187275_0004
  spark.decommissioning.timeout.threshold: 20
  # hive.metastore.uris: thrift://localhost:9083
  # hive.metastore.client.factory.class: com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory
  # hive.metastore.connect.retries: 3
