AWSTemplateFormatVersion: 2010-09-09
Description: Master of spark-3.1.1 cluster(Standalone).
Parameters:
  Ec2KeyName:
    Description: Name of an existing EC2 KeyPair to enable SSH access to EC2 instances.
    Type: AWS::EC2::KeyPair::KeyName
    ConstraintDescription: Must be the name of an existing EC2 KeyPair.
  InstanceType:
    Type: String
    Description: EC2 instance type.
    Default: t2.small
    AllowedValues:
      - t1.micro
      - t2.nano
      - t2.micro
      - t2.small
      - t2.medium
      - t2.large
      - m1.small
      - m1.medium
      - m1.large
      - m1.xlarge
      - m2.xlarge
      - m2.2xlarge
      - m2.4xlarge
      - m3.medium
      - m3.large
      - m3.xlarge
      - m3.2xlarge
      - m4.large
      - m4.xlarge
      - m4.2xlarge
      - m4.4xlarge
      - m4.10xlarge
      - m5.large
      - m5.xlarge
      - m5.2xlarge
      - m5.4xlarge
      - m5.8xlarge
      - m5.12xlarge
      - m5.16xlarge
      - c1.medium
      - c1.xlarge
      - c3.large
      - c3.xlarge
      - c3.2xlarge
      - c3.4xlarge
      - c3.8xlarge
      - c4.large
      - c4.xlarge
      - c4.2xlarge
      - c4.4xlarge
      - c4.8xlarge
      - g2.2xlarge
      - g2.8xlarge
      - r3.large
      - r3.xlarge
      - r3.2xlarge
      - r3.4xlarge
      - r3.8xlarge
      - i2.xlarge
      - i2.2xlarge
      - i2.4xlarge
      - i2.8xlarge
      - d2.xlarge
      - d2.2xlarge
      - d2.4xlarge
      - d2.8xlarge
      - hi1.4xlarge
      - hs1.8xlarge
      - cr1.8xlarge
      - cc2.8xlarge
  Ec2VolumeType:
    Type: String
    Default: gp2
    AllowedValues:
      - gp2
      - gp3
      - io1
      - io2
      - sc1
      - st1
      - standard
  Ec2VolumeSize:
    Type: Number
    Default: 30
    MinValue: 30
    MaxValue: 100
  S3Home:
    Description: S3 path for all needed jars and tars.
    Type: String
    Default: s3://chenyi-ap-southeast-1
  HadoopVersion:
    Description: Hadoop version.
    Type: String
    Default: 3.2.0
  HadoopFsS3Bucket:
    Description: S3 bucket for hadoop-2.10.1 file system.
    Type: String
    Default: chenyi-ap-southeast-1
  HadoopFsS3aEndpoint:
    Description: S3a endpoint for hadoop-2.10.1 file system.
    Type: String
    Default: s3.ap-southeast-1.amazonaws.com
  HiveVersion:
    Description: Hive version.
    Type: String
    Default: 2.3.9
  PrestoVersion:
    Description: Presto version.
    Type: String
    Default: 0.266.1
Mappings:
  AWSInstanceType2Arch:
    t1.micro:
      Arch: HVM64
    t2.nano:
      Arch: HVM64
    t2.micro:
      Arch: HVM64
    t2.small:
      Arch: HVM64
    t2.medium:
      Arch: HVM64
    t2.large:
      Arch: HVM64
    m1.small:
      Arch: HVM64
    m1.medium:
      Arch: HVM64
    m1.large:
      Arch: HVM64
    m1.xlarge:
      Arch: HVM64
    m2.xlarge:
      Arch: HVM64
    m2.2xlarge:
      Arch: HVM64
    m2.4xlarge:
      Arch: HVM64
    m3.medium:
      Arch: HVM64
    m3.large:
      Arch: HVM64
    m3.xlarge:
      Arch: HVM64
    m3.2xlarge:
      Arch: HVM64
    m4.large:
      Arch: HVM64
    m4.xlarge:
      Arch: HVM64
    m4.2xlarge:
      Arch: HVM64
    m4.4xlarge:
      Arch: HVM64
    m4.10xlarge:
      Arch: HVM64
    m5.large:
      Arch: HVM64
    m5.xlarge:
      Arch: HVM64
    m5.2xlarge:
      Arch: HVM64
    m5.4xlarge:
      Arch: HVM64
    m5.8xlarge:
      Arch: HVM64
    m5.12xlarge:
      Arch: HVM64
    m5.16xlarge:
      Arch: HVM64
    c1.medium:
      Arch: HVM64
    c1.xlarge:
      Arch: HVM64
    c3.large:
      Arch: HVM64
    c3.xlarge:
      Arch: HVM64
    c3.2xlarge:
      Arch: HVM64
    c3.4xlarge:
      Arch: HVM64
    c3.8xlarge:
      Arch: HVM64
    c4.large:
      Arch: HVM64
    c4.xlarge:
      Arch: HVM64
    c4.2xlarge:
      Arch: HVM64
    c4.4xlarge:
      Arch: HVM64
    c4.8xlarge:
      Arch: HVM64
    g2.2xlarge:
      Arch: HVMG2
    g2.8xlarge:
      Arch: HVMG2
    r3.large:
      Arch: HVM64
    r3.xlarge:
      Arch: HVM64
    r3.2xlarge:
      Arch: HVM64
    r3.4xlarge:
      Arch: HVM64
    r3.8xlarge:
      Arch: HVM64
    i2.xlarge:
      Arch: HVM64
    i2.2xlarge:
      Arch: HVM64
    i2.4xlarge:
      Arch: HVM64
    i2.8xlarge:
      Arch: HVM64
    d2.xlarge:
      Arch: HVM64
    d2.2xlarge:
      Arch: HVM64
    d2.4xlarge:
      Arch: HVM64
    d2.8xlarge:
      Arch: HVM64
    hi1.4xlarge:
      Arch: HVM64
    hs1.8xlarge:
      Arch: HVM64
    cr1.8xlarge:
      Arch: HVM64
    cc2.8xlarge:
      Arch: HVM64
  AWSRegionArch2AMI:
    ap-southeast-1:
      HVM64: ami-07191cf2912e097a6
      HVMG2: ami-0e46ce0d6a87dc979
    ap-southeast-2:
      HVM64: ami-0ae99b503e8694028
      HVMG2: ami-0c0ab057a101d8ff2
    ca-central-1:
      HVM64: ami-0803e21a2ec22f953
      HVMG2: NOT_SUPPORTED
    cn-north-1:
      HVM64: ami-07a3f215cc90c889c
      HVMG2: NOT_SUPPORTED
    cn-northwest-1:
      HVM64: ami-0a3b3b10f714a0ff4
      HVMG2: NOT_SUPPORTED
    eu-central-1:
      HVM64: ami-0474863011a7d1541
      HVMG2: ami-0aa1822e3eb913a11
    eu-north-1:
      HVM64: ami-0de4b8910494dba0f
      HVMG2: ami-32d55b4c
    eu-south-1:
      HVM64: ami-08427144fe9ebdef6
      HVMG2: NOT_SUPPORTED
    eu-west-1:
      HVM64: ami-015232c01a82b847b
      HVMG2: ami-0d5299b1c6112c3c7
    eu-west-2:
      HVM64: ami-0765d48d7e15beb93
      HVMG2: NOT_SUPPORTED
    eu-west-3:
      HVM64: ami-0caf07637eda19d9c
      HVMG2: NOT_SUPPORTED
    me-south-1:
      HVM64: ami-0744743d80915b497
      HVMG2: NOT_SUPPORTED
    sa-east-1:
      HVM64: ami-0a52e8a6018e92bb0
      HVMG2: NOT_SUPPORTED
    us-east-1:
      HVM64: ami-032930428bf1abbff
      HVMG2: ami-0aeb704d503081ea6
    us-east-2:
      HVM64: ami-027cab9a7bf0155df
      HVMG2: NOT_SUPPORTED
    us-west-1:
      HVM64: ami-088c153f74339f34c
      HVMG2: ami-0a7fc72dc0e51aa77
    us-west-2:
      HVM64: ami-01fee56b22f308154
      HVMG2: ami-0fe84a5b4563d8f27
Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId:
        Fn::FindInMap:
          - AWSRegionArch2AMI
          - !Ref AWS::Region
          - Fn::FindInMap:
              - AWSInstanceType2Arch
              - !Ref InstanceType
              - Arch
      InstanceType: !Ref InstanceType
      IamInstanceProfile:
        Fn::ImportValue: !Join [ ":", [ "Raven", "IAM", "InstanceProfile" ] ]
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref Ec2VolumeSize
            VolumeType: !Ref Ec2VolumeType
            DeleteOnTermination: true
      NetworkInterfaces:
        - DeviceIndex: 0
          AssociatePublicIpAddress: True
          DeleteOnTermination: True
          SubnetId:
            Fn::ImportValue: !Join [ ":", [ "Raven", "EC2", "Subnet01" ] ]
          GroupSet:
            - Fn::ImportValue: !Join [ ":", [ "Raven", "EC2", "SecurityGroup" ] ]
      Tags:
        - Key: Name
          Value: Presto Coordinator
      KeyName: !Ref Ec2KeyName
      UserData:
        Fn::Base64:
          Fn::Join:
            - ""
            - - |
                #!/bin/bash

                if [ ! ${USER} ]; then
                    USER=ec2-user
                fi

                if [ ! ${GROUP} ]; then
                    GROUP=ec2-user
                fi

                if [ ! ${HOME} ]; then
                    HOME=/home/${USER}
                fi

                cd ${HOME}
                touch shell.log && chown -R ${USER}:${GROUP} shell.log

                function info() {
                    log="$(date '+%Y-%m-%d %H:%M:%S') - INFO - $*"
                    echo -e "\033[32m${log}\033[0m"
                    echo "${log}" >> ${HOME}/shell.log
                }

                function warning() {
                    log="$(date '+%Y-%m-%d %H:%M:%S') - WARNING - $*"
                    echo -e "\033[33m${log}\033[0m"
                    echo "${log}" >> ${HOME}/shell.log
                }

                function error() {
                    log="$(date '+%Y-%m-%d %H:%M:%S') - ERROR - $*"
                    echo -e "\033[31m${log}\033[0m"
                    echo "${log}" >> ${HOME}/shell.log
                }

                function logging() {
                    level=$1
                    shift
                    case $level in
                    "info")
                        info "$*";;
                    "warning")
                        warning "$*";;
                    "error")
                        error "$*";;
                    *)
                        echo "$*";;
                    esac
                }
              - Fn::Sub:
                - |
                  S3_HOME=${S3Home}
                - S3Home: !Ref S3Home
              - |
                JDK_TARBALL=jdk-8u301-linux-x64.tar.gz
                JDK_DECOMPRESS_NAME=jdk1.8.0_301
                JAVA_HOME=${HOME}/java/${JDK_DECOMPRESS_NAME}
                JRE_HOME=${JAVA_HOME}/jre

                if java -version &> /dev/null; then
                    logging warning "Java has already been installed."
                    exit
                else
                    logging info "Java has not been installed."
                fi

                logging info "Installing Java SE 8..."
                mkdir -p ${HOME}/java && cd ${HOME}/java
                if [[ -f ${JDK_TARBALL} ]]; then
                    logging info "${JDK_TARBALL} has already been downloaded."
                else
                    logging info "Downloading ${JDK_TARBALL} from ${S3_HOME}/tars/${JDK_TARBALL}..."
                    aws s3 cp ${S3_HOME}/tars/${JDK_TARBALL} .
                    if [[ ! -f ${JDK_TARBALL} ]]; then
                        logging error "Failed to download ${JDK_TARBALL}."
                        exit 1
                    fi
                fi

                if [[ -d ${JAVA_HOME} ]]; then
                    logging info "${JDK_TARBALL} has already been decompressed."
                else
                    logging info "Decompressing ${JDK_TARBALL}..."
                    if ! tar -zxf ${JDK_TARBALL}; then
                	    logging error "Failed to decompress ${JDK_TARBALL}."
                	    exit 1
                    fi
                fi

                logging info "Setting up environment variables for java..."
                cat << EOF >> ${HOME}/.bash_profile

                # Java
                export JAVA_HOME=${JAVA_HOME}
                export JRE_HOME=\${JAVA_HOME}/jre
                export CLASSPATH=.:\${JAVA_HOME}/lib:\${JRE_HOME}/lib
                export PATH=\${PATH}:\${JAVA_HOME}/bin
                EOF
                source ${HOME}/.bash_profile

                if java -version &> /dev/null; then
                    logging info "Successfully installed java."
                else
                    logging error "Failed to install java."
                    exit 1
                fi

                chown -R ${USER}:${GROUP} ${HOME}/java
              - |
                if hadoop version &> /dev/null; then
                    logging warning "Hadoop already installed."
                    exit
                else
                    logging info "Hadoop has not been installed."
                fi
              - Fn::Sub:
                - |
                  FS_S3_BUCKET=${HadoopFsS3Bucket}
                  FS_S3A_ENDPOINT=${HadoopFsS3aEndpoint}

                  HADOOP_VERSION=${HadoopVersion}
                - HadoopVersion: !Ref HadoopVersion
                  HadoopFsS3Bucket: !Ref HadoopFsS3Bucket
                  HadoopFsS3aEndpoint: !Ref HadoopFsS3aEndpoint
              - |
                HADOOP_PACKAGE=hadoop-${HADOOP_VERSION}.tar.gz
                HADOOP_HOME=${HOME}/hadoop/hadoop-${HADOOP_VERSION}

                logging info "Start installing hadoop ${HADOOP_VERSION}..."
                mkdir -p ${HOME}/hadoop && cd ${HOME}/hadoop
                if [[ -f ${HADOOP_PACKAGE} ]]; then
                    logging info "${HADOOP_PACKAGE} has already been downloaded."
                else
                    logging info "Downloading ${HADOOP_PACKAGE} from AWS ${S3_HOME}/tars/${HADOOP_PACKAGE}..."

                    if ! aws s3 cp ${S3_HOME}/tars/${HADOOP_PACKAGE} .; then
                        logging error "Failed to download ${HADOOP_PACKAGE}."
                        exit 1
                    fi
                fi

                if [[ -d ${HADOOP_HOME} ]]; then
                    logging info "${HADOOP_PACKAGE} has already been decompressed."
                else
                    logging info "Decompressing ${HADOOP_PACKAGE}..."
                    if ! tar -zxf ${HADOOP_PACKAGE}; then
                        logging error "Failed to decompress ${HADOOP_PACKAGE}."
                        exit 1
                    fi
                fi

                ln -s ${HADOOP_HOME}/share/hadoop/tools/lib/*aws* ${HADOOP_HOME}/share/hadoop/common/lib/

                sudo chown -R ${USER}:${GROUP} ${HOME}/hadoop

                logging info "Setting up environment variables for hadoop..."
                cat << EOF >> ${HOME}/.bash_profile

                # Hadoop
                export HADOOP_HOME=${HADOOP_HOME}
                export PATH=${PATH}:\${HADOOP_HOME}/bin:\${HADOOP_HOME}/sbin
                EOF
                source ${HOME}/.bash_profile

                if hadoop version &> /dev/null; then
                    logging info "Successfully installed hadoop."
                else
                    logging error "Failed to install hadoop."
                    exit 1
                fi

                logging info "Modifying hadoop configurations..."
                cat << EOF > ${HADOOP_HOME}/etc/hadoop/core-site.xml
                <?xml version="1.0" encoding="UTF-8"?>
                <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
                <configuration>
                	<property>
                    <name>fs.default.name</name>
                    <value>s3a://${FS_S3_BUCKET}</value>
                	</property>
                  <property>
                    <name>fs.s3a.endpoint</name>
                    <value>${FS_S3A_ENDPOINT}</value>
                  </property>
                </configuration>
                EOF
              - Fn::Sub:
                - |
                  HIVE_VERSION=${HiveVersion}
                  MYSQL_HOST=${MysqlHost}
                  MYSQL_USER=${MysqlUser}
                  MYSQL_PASSWORD=${MysqlPassword}
                - HiveVersion: !Ref HiveVersion
                  MysqlHost:
                    Fn::ImportValue: !Join [ ":", [ "Raven", "Hive", "Metastore", "Host" ] ]
                  MysqlUser: hive
                  MysqlPassword: hive
              - |
                HIVE_PACKAGE=apache-hive-${HIVE_VERSION}-bin.tar.gz
                HIVE_DECOMPRESS_NAME=apache-hive-${HIVE_VERSION}-bin
                HIVE_HOME=${HOME}/hive/${HIVE_DECOMPRESS_NAME}
                AWS_SDK_VERSION=1.12.129

                MYSQL_CONNECTOR_PACKAGE=mysql-connector-java-5.1.40.jar

                if hive --version &> /dev/null; then
                    logging info "Hive has already been installed."
                else
                    logging info "Installing hive${HIVE_VERSION}..."
                fi

                mkdir -p ${HOME}/hive && cd ${HOME}/hive

                if [[ -f ${HIVE_PACKAGE} ]]; then
                    logging info "${HIVE_PACKAGE} has already been downloaded..."
                else
                    logging info "Downloading ${HIVE_PACKAGE} from ${S3_HOME}/tars/${HIVE_PACKAGE}..."
                    if ! aws s3 cp ${S3_HOME}/tars/${HIVE_PACKAGE} .; then
                        logging error "Failed to download ${HIVE_PACKAGE}."
                        exit 1
                    fi
                fi

                logging info "Decompressing ${HIVE_PACKAGE}..."
                if [[ -d ${HIVE_HOME} ]]; then
                    logging info "${HIVE_PACKAGE} has already been decompressed."
                else
                    if ! tar -zxf ${HIVE_PACKAGE}; then
                	    logging error "Failed to decompress ${HIVE_PACKAGE}"
                	    exit 1
                   fi
                fi

                if [[ -f ${MYSQL_CONNECTOR_PACKAGE} ]]; then
                    logging info "${MYSQL_CONNECTOR_PACKAGE} has already been downloaded..."
                else
                    logging info "Downloading ${MYSQL_CONNECTOR_PACKAGE} from ${S3_HOME}/tars/${MYSQL_CONNECTOR_PACKAGE}..."
                    if ! aws s3 cp ${S3_HOME}/jars/${MYSQL_CONNECTOR_PACKAGE} ${HIVE_HOME}/lib; then
                        logging error "Failed to download ${MYSQL_CONNECTOR_PACKAGE}."
                    fi
                    cp ${MYSQL_CONNECTOR_PACKAGE} ${HIVE_HOME}/lib
                fi

                if [ -n "$(sed -n -e '/HIVE_HOME/p' ${HOME}/.bash_profile)" ]; then
                    logging info "Hive environment variables have already been set."
                else
                    logging info "Setting up environment variables for hive..."
                    cat << EOF >> ${HOME}/.bash_profile

                # Hive
                export HIVE_HOME=${HIVE_HOME}
                export PATH=\${PATH}:\${HIVE_HOME}/bin
                EOF
                fi
                source ${HOME}/.bash_profile

                if hive --version &> /dev/null; then
                    logging info "Successfully installed hive."
                else
                    logging error "Failed to install hive."
                    exit 1
                fi

                logging info "Inorder to access AWS S3, downloading needed jars from ${S3_HOME}/jars..."

                if [ ! -f ${HIVE_HOME}/lib/aws-java-sdk-core-${AWS_SDK_VERSION}.jar ]; then
                    aws s3 cp ${S3_HOME}/jars/aws-java-sdk-core-${AWS_SDK_VERSION}.jar ${HIVE_HOME}/lib
                fi

                if [ ! -f ${HIVE_HOME}/lib/aws-java-sdk-s3-${AWS_SDK_VERSION}.jar ]; then
                    aws s3 cp ${S3_HOME}/jars/aws-java-sdk-s3-${AWS_SDK_VERSION}.jar ${HIVE_HOME}/lib
                fi

                if [ ! -f ${HIVE_HOME}/lib/hadoop-aws-${HADOOP_VERSION}.jar ]; then
                    aws s3 cp ${S3_HOME}/jars/hadoop-aws-${HADOOP_VERSION}.jar ${HIVE_HOME}/lib
                fi

                cp ${HIVE_HOME}/conf/hive-env.sh.template ${HIVE_HOME}/conf/hive-env.sh

                if [ -z "$(sed -n -e '/^export HIVE_AUX_JARS_PATH/p' ${HIVE_HOME}/conf/hive-env.sh)" ]; then
                    cat << EOF >> ${HIVE_HOME}/conf/hive-env.sh
                export HIVE_AUX_JARS_PATH=${HIVE_HOME}/lib/aws-java-sdk-core-${AWS_SDK_VERSION}.jar:${HIVE_HOME}/lib/aws-java-sdk-s3-${AWS_SDK_VERSION}.jar:${HIVE_HOME}/lib/hadoop-aws-${HADOOP_VERSION}.jar
                EOF
                fi

                logging info "Modifying hive configurations..."
                cat << EOF > ${HIVE_HOME}/conf/hive-site.xml
                <?xml version="1.0" encoding="UTF-8" standalone="no"?>
                <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
                <configuration>
                  <property>
                    <name>javax.jdo.option.ConnectionDriverName</name>
                    <value>com.mysql.jdbc.Driver</value>
                    <description>Driver class name for a JDBC metastore</description>
                  </property>
                  <property>
                    <name>javax.jdo.option.ConnectionURL</name>
                    <value>jdbc:mysql://${MYSQL_HOST}:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
                    <description>JDBC connect string for a JDBC metastore</description>
                  </property>
                  <property>
                    <name>javax.jdo.option.ConnectionUserName</name>
                    <value>${MYSQL_USER}</value>
                    <description>Username to use against metastore database;default is root</description>
                  </property>
                  <property>
                    <name>javax.jdo.option.ConnectionPassword</name>
                    <value>${MYSQL_PASSWORD}</value>
                    <description>password to use against metastore database</description>
                  </property>
                  <property>
                    <name>hive.metastore.schema.verification</name>
                    <value>false</value>
                  </property>
                  <property>
                    <name>hive.metastore.warehouse.dir</name>
                    <value>/user/hive/warehouse</value>
                  </property>
                  <property>
                    <name>hive.metastore.port</name>
                    <value>9083</value>
                  </property>
                  <property>
                    <name>hive.metastore.uris</name>
                    <value>thrift://${MYSQL_HOST}:9083</value>
                  </property>
                </configuration>
                EOF

                chown -R ${USER}:${GROUP} ${HOME}/hive
              - Fn::Sub:
                - |
                  PRESTO_VERSION=${PrestoVersion}
                - PrestoVersion: !Ref PrestoVersion
              - |
                PRESTO_PACKAGE=presto-server-${PRESTO_VERSION}.tar.gz
                PRESTO_CLIENT_JAR=presto-cli-${PRESTO_VERSION}-executable.jar
                PRESTO_HOME=${HOME}/presto/presto-server-${PRESTO_VERSION}

                logging info "Installing Presto ${PRESTO_VERSION}..."
                mkdir -p ${HOME}/presto && cd ${HOME}/presto

                if [ -f ${PRESTO_PACKAGE} ]; then
                    logging info "${PRESTO_PACKAGE} has already been downloaded."
                else
                    logging info "Downloading ${PRESTO_PACKAGE} from AWS ${S3_HOME}/tars/${PRESTO_PACKAGE}..."
                    if ! aws s3 cp ${S3_HOME}/tars/${PRESTO_PACKAGE} .; then
                        logging error "Failed to download ${PRESTO_PACKAGE}"
                        exit 1
                    fi
                fi

                if [ -d ${PRESTO_HOME} ]; then
                    logging "${PRESTO_PACKAGE} has already been decompressed."
                else
                    logging info "Decompressing ${PRESTO_PACKAGE}..."
                    tar -zxf ${PRESTO_PACKAGE}
                    if [ ! -d ${PRESTO_HOME} ]; then
                        logging error "Failed to decompress ${PRESTO_PACKAGE}."
                        exit 1
                    fi
                fi

                if [ -n "$(sed -n -e '/PRESTO_HOME/p' ${HOME}/.bash_profile)" ]; then
                    logging info "Presto environment variables have already been set."
                else
                    logging info "Setting up environment variables for presto."
                    cat << EOF >> ${HOME}/.bash_profile

                # Presto
                PRESTO_HOME=${PRESTO_HOME}
                PATH=\${PATH}:\${PRESTO_HOME}/bin
                EOF
                fi
                source ${HOME}/.bash_profile

                logging info "Modifying presto configurations..."
                mkdir -p ${PRESTO_HOME}/etc

                cat << EOF > ${PRESTO_HOME}/etc/config.properties
                coordinator=true
                node-scheduler.include-coordinator=true
                http-server.http.port=8080
                discovery-server.enabled=true
                discovery.uri=http://$(hostname):8080
                EOF

                cat << EOF > ${PRESTO_HOME}/etc/jvm.config
                -server
                -Xmx16G
                -XX:+UseG1GC
                -XX:G1HeapRegionSize=32M
                -XX:+UseGCOverheadLimit
                -XX:+ExplicitGCInvokesConcurrent
                -XX:+HeapDumpOnOutOfMemoryError
                -XX:+ExitOnOutOfMemoryError
                EOF

                cat << EOF > ${PRESTO_HOME}/etc/node.properties
                node.environment=production
                node.id=presto-coordinator-with-id-1
                node.data-dir=${PRESTO_HOME}/data
                EOF

                mkdir -p ${PRESTO_HOME}/etc/catalog

                cat << EOF > ${PRESTO_HOME}/etc/catalog/hive.properties
                connector.name=hive-hadoop2
                hive.metastore.uri=thrift://${MYSQL_HOST}:9083
                EOF

                cat << EOF > ${PRESTO_HOME}/etc/log.properties
                com.facebook.presto=INFO
                EOF

                logging info "Running presto server in background..."
                if su ${USER} -c "nohup ${PRESTO_HOME}/bin/launcher start"; then
                    logging info "Successfully start presto server."
                else
                    logging error "Failed to start presto server."
                fi

                logging info "Installing presto client..."
                if [ ! -f ${PRESTO_HOME}/bin/presto ]; then
                    aws s3 cp ${S3_HOME}/jars/${PRESTO_CLIENT_JAR} ${PRESTO_HOME}/bin/presto
                fi
                if [ ! -x ${PRESTO_HOME}/bin/presto ]; then
                    chmod u+x ${PRESTO_HOME}/bin/presto
                fi
                logging info "Successfully installed presto client."

                chown -R ${USER}:${GROUP} ${HOME}/presto

Outputs:
  Ec2InstanceId:
    Description: Unique ID of EC2 instance.
    Value: !Ref Ec2Instance
  Ec2InstanceAvailabilityZone:
    Description: The Availability Zone where the specified instance is launched.
    Value: !GetAtt Ec2Instance.AvailabilityZone
  Ec2InstancePublicIp:
    Description: Public IP of EC2 instance.
    Value: !GetAtt Ec2Instance.PublicIp
  Ec2InstancePublicDnsName:
    Description: The public DNS name of the specified instance.
    Value: !GetAtt Ec2Instance.PublicDnsName
  PrestoCoordinatorPrivateIp:
    Description: Private IP of presto coordinator.
    Value: !GetAtt Ec2Instance.PrivateIp
  PrestoWebUi:
    Description: Presto Web UI.
    Value: !Join
      - ""
      - - "http:"
        - !GetAtt Ec2Instance.PublicIp
        - ":8080"
